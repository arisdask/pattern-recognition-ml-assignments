{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition & Machine Learning Project\n",
    "\n",
    "*Authors*: Aristeidis Daskalopoulos (AEM: 10640), Georgios Rousomanis (AEM: 10703)\n",
    "\n",
    "----\n",
    "\n",
    "<center>\n",
    "This notebook contains the solution for Part D.\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "The goal of this part is to classify the feature vectors in `datasetTest.csv` into one of five classes ($\\omega_1, \\omega_2, \\ldots, \\omega_5$), using a model trained on `datasetTV.csv`. The training dataset contains $8743$ examples (*samples and their corresponding labels*), with each sample represented by a feature vector of dimentionality $224$. \n",
    "\n",
    "Our objective is to implement, validate, and test a model capable of achieving high accuracy in predicting the correct class for a given sample. \n",
    "\n",
    "To begin, we must first load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_path = \"datasetTV.csv\"\n",
    "data = np.loadtxt(file_path, delimiter=',')\n",
    "\n",
    "X = data[:, :-1]              # Extract the features\n",
    "y = data[:, -1].astype(int)   # Extract the labels\n",
    "\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Counts: {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in our dataset, the samples have nearly the same distribution across the classes (*almost equally distributed*). \n",
    "\n",
    "Before proceeding with building the model, it is important to perform ***basic data preprocessing***. This includes the following steps:\n",
    "\n",
    "- Scale the data so that it has zero mean and unit variance (**Standardization**).\n",
    "- Reduce the dimensionality of the data using *Principal Component Analysis* (PCA). This step should be done by determining the fraction of the total variance to be retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rnd_seed = 42\n",
    "n_splits = 5\n",
    "\n",
    "# Scale the data to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.plot(range(1, pca.explained_variance_ratio_.size + 1), np.cumsum(pca.explained_variance_ratio_), color='red')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram above, we can see that the Cumulative Explained Variance Ratio is a strictly increasing function without any \"elbow\" in the curve (so, the rate of increase in explained variance never slows down significantly). This indicates that all features contribute significantly to the classification process, and reducing them would result in the loss of important information.\n",
    "\n",
    "Now that we have performed a brief inspection of the data, we are ready to begin implementing our model. This notebook will follow a *step-by-step approach* to building the model, allowing us to track the thought process and decisions that lead to the final model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Inspection\n",
    "\n",
    "To better understand how separable our classes are, we will create a UMAP (*Uniform Manifold Approximation and Projection*) 2D plot of our training data. UMAP is a ***dimensionality reduction technique*** that is particularly well-suited for visualizing high-dimensional data in two (or three dimensions). It aims to preserve both the local and global structure of the data, making it a useful tool for exploring the relationships between samples in a dataset.\n",
    "\n",
    "The resulting UMAP visualization provides insights into the separability of the classes. If distinct clusters corresponding to different classes are visible, it suggests that the classes are ***well-separated*** in the feature space. Conversely, overlapping clusters may indicate that the classes are less distinguishable and could require *more advanced modeling techniques* to achieve accurate classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply UMAP\n",
    "umap_reducer = umap.UMAP(n_components=2, \n",
    "                         random_state=42, \n",
    "                         n_neighbors=15, \n",
    "                         min_dist=0.1, \n",
    "                         n_jobs=1)\n",
    "X_umap = umap_reducer.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the UMAP results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Class Labels')\n",
    "plt.title('UMAP Visualization')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above reveals several key insights about the separability of the classes:\n",
    "\n",
    "- Class $\\omega_1$ is almost completely distinct from the others, suggesting that it will be relatively *easy to classify*.  \n",
    "- Classes $3$ and $4$ exhibit some overlap, but it is not extreme, making their classification *moderately challenging*.  \n",
    "- Classes 2 and 5, however, are significantly intertwined in the 2D visualization, indicating that distinguishing between these two classes might be *particularly difficult*.\n",
    "\n",
    "These observations suggest that additional attention may be needed for classes 2 and 5. Specifically, a ***specialized model*** designed to differentiate between these two classes could improve overall classification performance.\n",
    "\n",
    "With this knowledge in mind, we can begin implementing simple classifiers.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first classifiers\n",
    "\n",
    "In this section, we introduce some classifiers we implemented to test our hypothesis about the classification difficulty for each class. Using different models independently allows us to evaluate their performance and identify which ones are worth pursuing further. By focusing on the models that achieve higher accuracy, we can refine our approach and discard those that do not perform well. \n",
    "\n",
    "*Below, we present the classifiers that yielded the highest accuracy in our initial experiments.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "####### Step 1: Data splitting\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.3,\n",
    "    stratify=y  # Ensure balanced split across all 5 classes\n",
    ")\n",
    "\n",
    "####### Step 2: Perform grid search to find optimal k\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 7, 10, 11, 12],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "base_knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    base_knn,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", f\"{grid_search.best_score_:.3f}\")\n",
    "\n",
    "####### Step 3: Get the best model and cross-validate\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(best_knn, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Average CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "####### Step 4: Train the model with best parameters and make predictions\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "y_pred = best_knn.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of KNN in test data: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "# Create figure with larger size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "disp.plot(cmap='Blues', values_format='.2%')\n",
    "plt.title('Confusion Matrix (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation of the steps in our KNN classification:*\n",
    "\n",
    "1. **Data Splitting**\n",
    "   - We take already scaled data (X_scaled) and split it into training and test sets\n",
    "   - Using 70/30 split ratio with stratification to maintain class distribution\n",
    "   - Stratification is particularly important since we have 5 classes\n",
    "\n",
    "2. **Hyperparameter Optimization**\n",
    "   - Using GridSearchCV to find the best parameters\n",
    "   - Testing different values e.g.:\n",
    "     - Number of neighbors (k)\n",
    "     - Weight functions\n",
    "     - Distance metrics\n",
    "   - Using 5-fold cross-validation during the search\n",
    "\n",
    "3. **Model Validation**\n",
    "   - Taking the best model from grid search\n",
    "   - Performing 5-fold cross-validation to assess model stability\n",
    "   - Reporting mean accuracy and standard deviation\n",
    "\n",
    "4. **Final Training and Evaluation**\n",
    "   - Training the best model on the full training set\n",
    "   - Making predictions on test data\n",
    "   - Evaluating performance using:\n",
    "     - Overall accuracy score\n",
    "     - Normalized confusion matrix visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next classifiers, we will use them with preset parameters (without performing any hyperparameter tuning) and simply evaluate their accuracy on a test set.  \n",
    "\n",
    "The purpose of this step is to gain an initial understanding of the underlying patterns in the data. By implementing simple but efficient models (with a total accuracy of more than 80%), we can identify which classifiers are promising and worth further exploration.  \n",
    "\n",
    "This approach allows us to focus on the *general behavior* of the models without getting stucked in parameter optimization at this stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.3,\n",
    "    stratify=y  # Ensure balanced split across all 5 classes\n",
    ")\n",
    "\n",
    "# Define base estimator (KNN)\n",
    "clf = RandomForestClassifier(n_estimators=100, bootstrap=True)\n",
    "\n",
    "# Train BaggingClassifier\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Random Forest Classifier: {accuracy}\")\n",
    "\n",
    "# Compute and normalize the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')  # Normalize by true label count\n",
    "\n",
    "# Customize the ConfusionMatrixDisplay for percentages\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "disp.plot(cmap='Blues', values_format=\".2%\")  # Format as percentages\n",
    "plt.title('Confusion Matrix (Percentages)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.3,\n",
    "    stratify=y  # Ensure balanced split across all 5 classes\n",
    ")\n",
    "\n",
    "# Initialize the MLP classifier\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 30),  # Two hidden layers with 50 and 30 neurons\n",
    "    activation='relu',                    # Activation function for hidden layers\n",
    "    solver='adam',                        # Optimization algorithm\n",
    "    max_iter=500,                         # Maximum number of iterations\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy for MLP: {accuracy}\")\n",
    "\n",
    "# Compute and normalize the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')  # Normalize by true label count\n",
    "\n",
    "# Customize the ConfusionMatrixDisplay for percentages\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
    "disp.plot(cmap='Blues', values_format=\".2%\")  # Format as percentages\n",
    "plt.title('Confusion Matrix (Percentages)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of iterations for averaging\n",
    "num_iterations = 1\n",
    "accumulated_cm = None\n",
    "total_accuracy = 0\n",
    "\n",
    "# Calculate balanced class weights based on the class distribution\n",
    "class_counts = [1769, 1720, 1754, 1716, 1784]\n",
    "n_samples = sum(class_counts)\n",
    "class_weights = {i+1: n_samples/(len(class_counts) * count) for i, count in enumerate(class_counts)}\n",
    "\n",
    "# Hyperparameters for SVM\n",
    "svm_params = {\n",
    "    'C': 10,\n",
    "    'kernel': 'rbf',  # Radial basis function kernel\n",
    "    'class_weight': class_weights,\n",
    "    'gamma': 'scale',  # Automatically scales gamma based on n_features * X.var()\n",
    "    'probability': True  # Enable probability estimates\n",
    "}\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Split and preprocess data\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, \n",
    "        test_size=0.3,\n",
    "        stratify=y  # Ensure balanced split across all 5 classes\n",
    "    )\n",
    "    \n",
    "    # Train and predict\n",
    "    clf = SVC(**svm_params)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy and confusion matrix\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "    # Compute normalized confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    \n",
    "    # Accumulate confusion matrices\n",
    "    if accumulated_cm is None:\n",
    "        accumulated_cm = cm\n",
    "    else:\n",
    "        accumulated_cm += cm\n",
    "\n",
    "# Calculate averages\n",
    "average_cm = accumulated_cm / num_iterations\n",
    "average_accuracy = total_accuracy / num_iterations\n",
    "\n",
    "print(f\"Average SVM Accuracy: {average_accuracy:.4f}\")\n",
    "\n",
    "# Visualize the average confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm, \n",
    "                             display_labels=np.arange(1, 6))  # Labels 1-5\n",
    "disp.plot(cmap='Blues', values_format='.2%')\n",
    "plt.title('Average SVM Confusion Matrix (Percentages)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results align with our initial hypothesis:  \n",
    "\n",
    "- Classes $\\omega_1$, $\\omega_2 + \\omega_5$, and $\\omega_3 + \\omega_4$ are relatively easy to classify.  \n",
    "- With a properly tuned SVM (Support Vector Machine), we propably can also achieve a decent separation between $\\omega_3$ and $\\omega_4$.  \n",
    "- However, distinguishing between $\\omega_2$ and $\\omega_5$ remains a challenge.  \n",
    "\n",
    "Based on our experiments, we conclude that an SVM is an excellent initial choice for the main classification model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model:\n",
    "\n",
    "1. Use an SVM to classify the data into the following categories:  \n",
    "   - $\\omega_1$  \n",
    "   - $\\omega_2 + \\omega_5$  \n",
    "   - $\\omega_3$  \n",
    "   - $\\omega_4$  \n",
    "\n",
    "2. After this initial classification, employ a second, specialized classifier trained specifically to differentiate between $\\omega_2$ and $\\omega_5$.  \n",
    "\n",
    "This two-step approach leverages the strengths of the SVM for general classification and refines performance with a focused model for the more challenging classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM With Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flag to toggle hyperparameter tuning\n",
    "tune_params = False\n",
    "\n",
    "####### Step 0: Function to merge classes\n",
    "def merge_classes(y):\n",
    "    y_merged = y.copy()\n",
    "    y_merged[y_merged == 5] = 2  # Merge class 5 into 2\n",
    "    return y_merged\n",
    "\n",
    "# Get the merged classes for initial class weight calculation\n",
    "y_merged = merge_classes(y)\n",
    "unique_classes, class_counts = np.unique(y_merged, return_counts=True)\n",
    "n_samples = sum(class_counts)\n",
    "class_weights = {label: n_samples / (len(unique_classes) * count)\n",
    "                 for label, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "####### Step 1: Data splitting\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.3,\n",
    "    stratify=y_merged  # Ensure balanced split across all classes\n",
    ")\n",
    "\n",
    "y_train_init = y_train\n",
    "y_test_init = y_test\n",
    "\n",
    "# Merge classes in both training and test sets\n",
    "y_train = merge_classes(y_train)\n",
    "y_test = merge_classes(y_test)\n",
    "\n",
    "####### Step 2: Select parameters (fixed or tuned)\n",
    "if tune_params:\n",
    "    # Perform grid search to find optimal parameters\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "    }\n",
    "    base_svm = SVC(kernel='rbf',\n",
    "                   probability=True,\n",
    "                   class_weight={1:3, 2:1, 3:3, 4:3},\n",
    "                   random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        base_svm,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # Print best parameters\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score: \", f\"{grid_search.best_score_:.3f}\")\n",
    "    # Get the best model\n",
    "    best_svm = grid_search.best_estimator_\n",
    "else:\n",
    "    # Use fixed values for C and gamma\n",
    "    fixed_C = 10\n",
    "    fixed_gamma = 'scale'\n",
    "    best_svm = SVC(kernel='rbf',\n",
    "                   C=fixed_C,\n",
    "                   gamma=fixed_gamma,\n",
    "                   probability=True,\n",
    "                   class_weight={1:3, 2:1, 3:3, 4:3},\n",
    "                   random_state=42)\n",
    "\n",
    "####### Step 3: Cross-validate and train the model\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(best_svm, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Average CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Train the model with selected parameters\n",
    "best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "####### Step 4: Make predictions and evaluate\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of SVM on test data: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "plt.figure(figsize=(12, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "disp.plot(cmap='Blues', values_format='.2%')\n",
    "plt.title('Confusion Matrix (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(y_test_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we observe that splitting the classes into $\\omega_1$, $\\omega_2' = \\omega_2 + \\omega_5$, $\\omega_3$, and $\\omega_4$ can be effectively achieved using an SVM. However, we also notice that many samples (more than 5%) from the class $\\omega_4$ are misclassified as belonging to $\\omega_2'$.  \n",
    "\n",
    "To address this issue, we propose introducing an additional binary SVM classifier to refine the classification process:  \n",
    "1. **Step 1**: Use the initial SVM to classify a sample into one of the primary categories ($\\omega_1$, $\\omega_2'$, $\\omega_3$, or $\\omega_4$).  \n",
    "2. **Step 2**: ***If a sample is classified as $\\omega_2'$***:  \n",
    "   - Use a binary SVM to determine if the sample belongs to $\\omega_4$. This binary SVM should be able to be more precise while predicting the class $\\omega_4$.\n",
    "   - If the sample ***does not*** belong to $\\omega_4$, we assume we made the classification correct, otherwise we change our estimation to $\\omega_4$.\n",
    "\n",
    "This hierarchical approach allows us to iteratively refine the classification process, ensuring higher accuracy for misclassified samples and improving overall performance. To run the next code, make sure you already run the previous code cell first:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def merge_classes(y):\n",
    "    y_merged = y.copy()\n",
    "    y_merged[y_merged == 5] = 2\n",
    "    return y_merged\n",
    "\n",
    "def get_binary_subset(X, y):\n",
    "    \"\"\"Extract only samples belonging to classes 2 and 4\"\"\"\n",
    "    mask = (y == 2) | (y == 4)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "class ImprovedClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, svm_model1):\n",
    "        self.svm_model1 = svm_model1\n",
    "        # Initialize second SVM with stronger class weights\n",
    "        self.svm_model2 = SVC(kernel='rbf',\n",
    "                             C=100,\n",
    "                             gamma='scale',\n",
    "                             probability=True,\n",
    "                             # Make misclassifying class 4 even more costly\n",
    "                             class_weight={2: 1, 4: 3},\n",
    "                             random_state=42)\n",
    "        # Initialize PCA\n",
    "        self.pca = PCA(n_components=0.80)  # Keep 95% of variance\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Get only class 2 and 4 samples for training second SVM\n",
    "        X_binary, y_binary = get_binary_subset(X, y)\n",
    "        \n",
    "        # Fit and transform PCA on binary subset\n",
    "        X_binary_pca = self.pca.fit_transform(X_binary)\n",
    "        print(f\"Number of components selected by PCA: {X_binary_pca.shape[1]}\")\n",
    "        \n",
    "        # Fit the second SVM on PCA-transformed data\n",
    "        if len(np.unique(y_binary)) > 1:\n",
    "            self.svm_model2.fit(X_binary_pca, y_binary)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Get initial predictions from first SVM\n",
    "        initial_predictions = self.svm_model1.predict(X)\n",
    "        \n",
    "        # Only double-check samples predicted as class 2\n",
    "        class_2_mask = (initial_predictions == 2)\n",
    "        final_predictions = initial_predictions.copy()\n",
    "        \n",
    "        if np.any(class_2_mask):\n",
    "            # Get subset of samples predicted as class 2\n",
    "            X_subset = X[class_2_mask]\n",
    "            \n",
    "            # Transform subset using fitted PCA\n",
    "            X_subset_pca = self.pca.transform(X_subset)\n",
    "            \n",
    "            # Get probability estimates from second SVM\n",
    "            prob_predictions = self.svm_model2.predict_proba(X_subset_pca)\n",
    "            \n",
    "            # Get the probability for class 4\n",
    "            class_4_probs = prob_predictions[:, 1]  # Assuming class 4 is index 1\n",
    "            \n",
    "            # Use an even lower threshold for class 4 to catch more of them\n",
    "            class_4_mask = class_4_probs > 0.3  # More aggressive threshold\n",
    "            \n",
    "            # Create array of predictions based on probabilities\n",
    "            secondary_predictions = np.where(class_4_mask, 4, 2)\n",
    "            \n",
    "            # Update predictions\n",
    "            final_predictions[class_2_mask] = secondary_predictions\n",
    "            \n",
    "        return final_predictions\n",
    "\n",
    "def evaluate_improved_classifier(X_train_scaled, X_test_scaled, y_train, y_test, svm_model1):\n",
    "    # Initialize and fit the improved classifier\n",
    "    improved_clf = ImprovedClassifier(svm_model1)\n",
    "    improved_clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_improved = improved_clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_improved)\n",
    "    print(f\"Accuracy of Improved Classifier on test data: {accuracy:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    display_labels = sorted(np.unique(y_test))\n",
    "    cm = confusion_matrix(y_test, y_pred_improved, \n",
    "                         labels=display_labels, normalize='true')\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                 display_labels=display_labels)\n",
    "    disp.plot(cmap='Blues', values_format='.2%')\n",
    "    plt.title('Confusion Matrix - Improved Classifier (Normalized)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_improved))\n",
    "    \n",
    "    # Analyze specifically class 2/4 confusion\n",
    "    mask_2_4 = (y_test == 2) | (y_test == 4)\n",
    "    if np.any(mask_2_4):\n",
    "        print(\"\\nDetailed analysis of Class 2 vs 4 predictions:\")\n",
    "        y_test_binary = y_test[mask_2_4]\n",
    "        y_pred_binary = y_pred_improved[mask_2_4]\n",
    "        print(classification_report(y_test_binary, y_pred_binary))\n",
    "    \n",
    "    return y_pred_improved\n",
    "\n",
    "# Create and evaluate the improved classifier\n",
    "y_pred_final = evaluate_improved_classifier(\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, best_svm\n",
    ")\n",
    "\n",
    "classes, counts = np.unique(y_pred_final, return_counts=True)\n",
    "print(f'\\nFinal prediction distribution:')\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Counts: {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident, we have successfully addressed the challenge of misclassified class $\\omega_4$, improving the accuracy of the model and ensuring better classification performance for this class.\n",
    "\n",
    "The remaining challenge is distinguishing between classes 2 and 5, which are combined into $\\omega_2 + \\omega_5$ during the first classification step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Solution for Distinguishing $\\omega_2$ and $\\omega_5$\n",
    "\n",
    "As mentioned earlier, a simple and efficient way to address this issue is by introducing a new binary classifier. This binary classifier will be used specifically when the SVM from the first step classifies a sample as belonging to $\\omega_2 + \\omega_5$ (and after we check that this sample does not belong to class $\\omega_4$).\n",
    "\n",
    "The goal of this secondary classifier is to achieve higher accuracy in distinguishing $\\omega_2$ from $\\omega_5$ than the primary SVM, even after hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the binary classifiers, we will first explore various transformations of the feature vectors to evaluate whether these transformations can improve the separability of classes 2 and 5.  \n",
    "\n",
    "By analyzing these transformations, we aim to identify patterns or relationships within the data that might enhance the performance of the binary classifier. This step helps us better understand the data and ensures we leverage the most effective feature representations for distinguishing between the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "####### Step 0: Function to exclude and merge classes\n",
    "def preprocess_classes(X, y):\n",
    "    # Exclude class 1\n",
    "    mask = y != 1  # Keep all samples not belonging to class 1\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "    \n",
    "    # Merge classes 3 and 2 into class 2\n",
    "    y_filtered = np.where(y_filtered == 3, 2, y_filtered)\n",
    "    # Merge classes 2 and 5 into class 2\n",
    "    y_filtered = np.where(y_filtered == 5, 2, y_filtered)\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed, y_preprocessed = preprocess_classes(X_scaled, y)\n",
    "\n",
    "# Initialize UMAP\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "\n",
    "# List of the different transformations\n",
    "transformations = {\n",
    "    \"Polynomial Features\": PolynomialFeatures(degree=2, include_bias=False),\n",
    "    \"Interaction Features\": PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "    \"t-SNE\": TSNE(n_components=2, random_state=42),\n",
    "    \"PCA\": PCA(n_components=0.95),\n",
    "    \"Mutual Info Selection\": SelectKBest(mutual_info_classif, k=50),\n",
    "    \"RFE (SVC)\": RFE(estimator=SVC(kernel='linear'), n_features_to_select=50, step=1)\n",
    "}\n",
    "\n",
    "# Create a figure for the plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Loop through transformations and apply UMAP, then plot\n",
    "for i, (name, transformer) in enumerate(transformations.items()):\n",
    "    # Transform the data\n",
    "    if name == \"t-SNE\":\n",
    "        X_transformed = transformer.fit_transform(X_preprocessed)  # For t-SNE, we use it directly\n",
    "    else:\n",
    "        X_transformed = transformer.fit_transform(X_preprocessed, y_preprocessed)\n",
    "    \n",
    "    # Apply UMAP to the transformed data\n",
    "    X_umap = umap_model.fit_transform(X_transformed)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.scatter(X_umap[:, 0], X_umap[:, 1], c=y_preprocessed, cmap='viridis', edgecolors='k')\n",
    "    ax.set_title(f\"UMAP - {name}\")\n",
    "    ax.set_xlabel(\"UMAP 1\")\n",
    "    ax.set_ylabel(\"UMAP 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, keep all existing code unchanged\n",
    "# Then add the following new code:\n",
    "\n",
    "class FinalStageClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, improved_classifier):\n",
    "        self.improved_classifier = improved_classifier\n",
    "        # Initialize third SVM specifically for class 2 vs 5\n",
    "        self.svm_model3 = SVC(kernel='rbf',\n",
    "                              C=100,\n",
    "                              gamma='scale',\n",
    "                              probability=True,\n",
    "                              random_state=42)\n",
    "        # Initialize PCA for 2 vs 5 separation\n",
    "        self.pca3 = PCA(n_components=0.85)\n",
    "        self.has_2_5_classifier = False\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # First fit the improved classifier (1,2+5,3,4)\n",
    "        self.improved_classifier.fit(X, y)\n",
    "        \n",
    "        # Prepare data for 2 vs 5 classifier\n",
    "        mask_2_5 = (y == 2) | (y == 5)\n",
    "        if np.any(mask_2_5):\n",
    "            X_2_5 = X[mask_2_5]\n",
    "            y_2_5 = y[mask_2_5]\n",
    "            \n",
    "            if len(np.unique(y_2_5)) > 1:\n",
    "                # Transform data using PCA\n",
    "                X_2_5_pca = self.pca3.fit_transform(X_2_5)\n",
    "                print(f\"Number of components selected by PCA (2 vs 5): {X_2_5_pca.shape[1]}\")\n",
    "                \n",
    "                # Create binary labels (2=0, 5=1)\n",
    "                y_2_5_binary = (y_2_5 == 5).astype(int)\n",
    "                self.svm_model3.fit(X_2_5_pca, y_2_5_binary)\n",
    "                self.has_2_5_classifier = True\n",
    "            else:\n",
    "                print(\"Warning: Not enough classes for 2 vs 5 classifier\")\n",
    "                self.has_2_5_classifier = False\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Get predictions from improved classifier (1,2+5,3,4)\n",
    "        predictions = self.improved_classifier.predict(X)\n",
    "        \n",
    "        # Only process samples predicted as class 2\n",
    "        if self.has_2_5_classifier:\n",
    "            class_2_mask = (predictions == 2)\n",
    "            if np.any(class_2_mask):\n",
    "                X_subset = X[class_2_mask]\n",
    "                X_subset_pca = self.pca3.transform(X_subset)\n",
    "                \n",
    "                # Get probabilities for class 5\n",
    "                proba_2_5 = self.svm_model3.predict_proba(X_subset_pca)\n",
    "                class_5_probs = proba_2_5[:, 1]\n",
    "                \n",
    "                # Use probability threshold to decide class 5\n",
    "                class_5_mask = class_5_probs > 0.5\n",
    "                predictions[class_2_mask] = np.where(class_5_mask, 5, 2)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def evaluate_complete_pipeline(X_train_scaled, X_test_scaled, y_train, y_test, svm_model1):\n",
    "    # Create the complete pipeline\n",
    "    improved_clf = ImprovedClassifier(svm_model1)\n",
    "    final_clf = FinalStageClassifier(improved_clf)\n",
    "    \n",
    "    # Fit the complete pipeline\n",
    "    final_clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_final = final_clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_final)\n",
    "    print(f\"\\nAccuracy of Complete Pipeline on test data: {accuracy:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    display_labels = sorted(np.unique(y_test))\n",
    "    cm = confusion_matrix(y_test, y_pred_final, \n",
    "                          labels=display_labels, normalize='true')\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                  display_labels=display_labels)\n",
    "    disp.plot(cmap='Blues', values_format='.2%')\n",
    "    plt.title('Confusion Matrix - Complete Pipeline (Normalized)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed report\n",
    "    print(\"\\nComplete Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_final))\n",
    "    \n",
    "    # Analyze specifically class 2/5 confusion\n",
    "    mask_2_5 = (y_test == 2) | (y_test == 5)\n",
    "    if np.any(mask_2_5):\n",
    "        print(\"\\nDetailed analysis of Class 2 vs 5 predictions:\")\n",
    "        y_test_binary = y_test[mask_2_5]\n",
    "        y_pred_binary = y_pred_final[mask_2_5]\n",
    "        print(classification_report(y_test_binary, y_pred_binary))\n",
    "    \n",
    "    return y_pred_final\n",
    "\n",
    "# Run the complete pipeline\n",
    "y_pred_final = evaluate_complete_pipeline(\n",
    "    X_train_scaled, X_test_scaled, y_train_init, y_test_init, best_svm\n",
    ")\n",
    "\n",
    "# Show final distribution\n",
    "classes, counts = np.unique(y_pred_final, return_counts=True)\n",
    "print(f'\\nFinal prediction distribution:')\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Counts: {counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_file_path = \"datasetTV.csv\"\n",
    "train_data = np.loadtxt(train_file_path, delimiter=',')\n",
    "X_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1].astype(int)\n",
    "\n",
    "# Load testing data\n",
    "test_file_path = \"datasetTest.csv\"\n",
    "test_data = np.loadtxt(test_file_path, delimiter=',')\n",
    "X_test = test_data[:]\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "\n",
    "# Calculate balanced class weights based on the class distribution\n",
    "class_counts = [1769, 1720, 1754, 1716, 1784]\n",
    "n_samples = sum(class_counts)\n",
    "class_weights = {i+1: n_samples/(len(class_counts) * count) for i, count in enumerate(class_counts)}\n",
    "\n",
    "# Hyperparameters for SVM\n",
    "svm_params = {\n",
    "    'C': 10,\n",
    "    'kernel': 'rbf',  # Radial basis function kernel\n",
    "    'class_weight': class_weights,\n",
    "    'gamma': 'scale',  # Automatically scales gamma based on n_features * X.var()\n",
    "    'probability': True  # Enable probability estimates\n",
    "}\n",
    "    \n",
    "# Train and predict\n",
    "clf = SVC(**svm_params)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print(f'y_pred shape: {y_pred}')\n",
    "print(y_pred)\n",
    "\n",
    "# Save y_pred to a .npy file\n",
    "output_file = \"labels63.npy\"\n",
    "np.save(output_file, y_pred)\n",
    "\n",
    "print(f'y_pred has been saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_labels = np.load(\"labels63.npy\")\n",
    "print(loaded_labels.shape)\n",
    "print(loaded_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
