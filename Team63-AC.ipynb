{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition & Machine Learning Project\n",
    "\n",
    "*Authors*: Aristeidis Daskalopoulos (AEM: 10640), Georgios Rousomanis (AEM: 10703)\n",
    "\n",
    "----\n",
    "\n",
    "<center>\n",
    "This notebook contains the solutions for Parts A, B, and C.\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "### Theoretical Analysis\n",
    "\n",
    "In this part, we address a **binary classification problem** where the goal is to classify samples into one of two classes: $\\omega_1$ or $\\omega_2$, based on *a single feature* $x$ (a feature vector of dimensionality one).\n",
    "\n",
    "To achieve this, we use the probability density function (PDF) of the feature $x$, which follows - for both classes - the distribution described below:\n",
    "\n",
    "$$ p(x|\\theta) = \\frac{1}{\\pi}\\frac{1}{1 + (x - \\theta)^2} \\quad, $$\n",
    "\n",
    "where $\\theta$ is an unknown parameter which has to be defined for each one of the classes separately. This PDF is the probability distribution of the [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution) for $\\gamma = 1$. \n",
    "\n",
    "To solve this decision problem, we will implement a Generative Probabilistic Model, following the steps described in the subsequent cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Our first goal is to estimate the parameters $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$ using the Maximum Likelihood (ML) method. To do this, we aim to maximize the log-likelihood function with respect to $\\theta_j$, for $j = 1, 2$; $\\theta_1$ is the parameter of the first class, and $\\theta_2$ refers to class $\\omega_2$.\n",
    " \n",
    "\n",
    "Assuming that the samples $D_j$ of the class $\\omega_j$, for $j = 1, 2$, are ***independent and identically distributed (i.i.d.)***, meaning they have been drawn independently from the same distribution $p(x | \\theta_j, \\omega_j)$, the probability density function (PDF) for the samples can be expressed as:\n",
    "\n",
    "$$ p(D_j | \\theta_j) = \\prod_{i=1}^{N_j} p(x_i | \\theta_j)\\quad.$$\n",
    "\n",
    "We prefer to work with the log-likelihood function, because it simplifies the process - as it converts multiplication into addition, which is *less* error-sensitive in terms of computational arithmetic errors (and in terms of calculating derivates). The log-likelihood of our problem is:\n",
    "\n",
    "$$ l(\\theta_j) = \\log p(D_j | \\theta_j) = \\sum_{i=1}^{N_j} \\log p(x_i | \\theta_j), \\quad j = 1, 2 \\quad \\Rightarrow$$\n",
    "\n",
    "\n",
    "$$ l(\\theta_j) = \\sum_{i=1}^{N_j} \\log \\left(\\frac{1}{\\pi}\\frac{1}{1 + (x_i - \\theta_j)^2}\\right), \\quad j = 1, 2 \\quad \\Rightarrow$$\n",
    "\n",
    "$$ l(\\theta_j) = - N_j \\cdot \\log \\pi - \\sum_{i=1}^{N_j} \\log (1 + (x_i - \\theta_j)^2), \\quad j = 1, 2 \\quad,$$\n",
    "\n",
    "with which we can estimate the $\\hat{\\theta}_j$ for each class. This estimate, $\\hat{\\theta}_j$, is by definition the value of $\\theta_j$ that maximizes the likelihood/log-likelihood. In our case the term $- N_j \\cdot \\log \\pi$ is constant, so we practically just need to *minimize* the term $\\sum_{i=1}^{N_j} \\log (1 + (x_i - \\theta_j)^2)$.\n",
    "\n",
    "One approach to solving this problem is to calculate the derivatives and solve the following equations, where the solution gives the estimate $\\hat{\\theta}_j$ for each class:\n",
    "\n",
    "$$ \\frac{d}{d\\theta_j} l(\\theta_j) = 0 \\quad \\Rightarrow \\quad \\frac{d}{d\\theta_j} \\left(- N_j \\cdot \\log \\pi - \\sum_{i=1}^{N_j} \\log (1 + (x_i - \\theta_j)^2)\\right) = 0 \\quad \\Rightarrow$$\n",
    "\n",
    "$$ \\sum_{i=1}^{N_j} \\frac{d}{d\\theta_j}\\log (1 + (x_i - \\theta_j)^2) = 0 \\quad \\Rightarrow$$\n",
    "\n",
    "$$ \\sum_{i=1}^{N_j} \\frac{-2 \\cdot (x_i - \\theta_j)}{1 + (x_i - \\theta_j)^2} = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{N_j} \\frac{x_i - \\theta_j}{1 + (x_i - \\theta_j)^2} = 0 \\quad.$$\n",
    "\n",
    "*For all the $\\hat{\\theta}_j$ that solve the above equation we should choose the one that gives the largest (max) value to the $l(\\theta_j)$.*\n",
    "\n",
    "Given $l(\\theta_j)$, its derivative can be computed efficiently (e.g., using a library like SymPy) to solve the equation and obtain the estimate $\\hat{\\theta}_j$. However, by plotting $l(\\theta_j)$ as requested, we inherently *calculate the values of the log-likelihood function across multiple points*. Consequently, selecting the value of $\\theta$ that maximizes $l(\\theta)$ provides the same solution, thereby **avoiding** the need for the derivative-based approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Bayes Decision Rule\n",
    "\n",
    "Using the Bayes Decision Rule, we classify to $\\omega_1$ based on the following condition:\n",
    "\n",
    "$$ P(\\omega_1 | x) > P(\\omega_2 | x) \\quad, $$\n",
    "\n",
    "which can be rewritten using the *Bayes formula* as:\n",
    "\n",
    "$$ \\frac{p(x|\\omega_1) P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) P(\\omega_2)}{p(x)} \\quad, $$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$$ \\log p(x|\\omega_1) + \\log P(\\omega_1) > \\log p(x|\\omega_2) + \\log P(\\omega_2) \\quad. $$\n",
    "\n",
    "*Here, the class-conditional densities $p(x|\\omega_1, \\theta_1)$ and $p(x|\\omega_2, \\theta_2)$ have been fully defined using Maximum Likelihood (ML) estimation, for the parameters $\\theta_1 = \\hat{\\theta}_1$ and $\\theta_2 = \\hat{\\theta}_2$ respectively.* So, for each class we have that:\n",
    "\n",
    "$$ p(x|\\omega_j) = \\frac{1}{\\pi}\\frac{1}{1 + (x - \\hat{\\theta}_j)^2}, \\quad P(\\omega_j) = \\frac{||D_j||}{||D_1|| + ||D_2||}, $$\n",
    "\n",
    "where $||D_j||$ is the total number of elements, $N_j$, that this dataset has.\n",
    "\n",
    "We define the following discriminant function:\n",
    "\n",
    "$$ g(x) = \\log p(x|\\omega_1) - \\log p(x|\\omega_2) + \\log P(\\omega_1) - \\log P(\\omega_2) \\quad, $$\n",
    "\n",
    "and based on the previous inequity we infer that using this discriminant function:\n",
    "- If $g(x) > 0$, the sample with feature $x$ is classified into class $\\omega_1$.\n",
    "- Otherwise, it is classified into class $\\omega_2$.\n",
    "\n",
    "The above **rule** implies that we theoretically expect the discriminant function $g(x)$ to be greater than zero when a sample from the $D_1$ set (class $\\omega_1$) is provided. Based on this rule, the feature space - represented by the real number line $\\mathbb{R}$ - is divided into two distinct regions: $\\mathbb{R}_1$ and $\\mathbb{R}_2$. To complete the theoretical analysis of this section, these regions must be defined by determining their boundaries, which can be found by solving $g(x) = 0$:\n",
    "\n",
    "\n",
    "$$ \\log(\\frac{1}{\\pi}\\frac{1}{1 + (x - \\hat{\\theta}_1)^2}) - \\log(\\frac{1}{\\pi}\\frac{1}{1 + (x - \\hat{\\theta}_2)^2}) + \\log(\\frac{||D_1||}{||D_1|| + ||D_2||}) - \\log(\\frac{||D_2||}{||D_1|| + ||D_2||}) = 0 \\quad \\Rightarrow$$\n",
    "\n",
    "$$ -\\log(1 + (x - \\hat{\\theta}_1)^2) + \\log(1 + (x - \\hat{\\theta}_2)^2) + \\log(\\frac{||D_1||}{||D_2||}) = 0, \\quad Let\\ r = \\frac{||D_1||}{||D_2||} \\quad \\Rightarrow$$\n",
    "\n",
    "$$ \\log(\\frac{1 + (x - \\hat{\\theta}_2)^2}{1 + (x - \\hat{\\theta}_1)^2}) = -\\log(r) \\quad \\Rightarrow \\quad \\frac{1 + (x - \\hat{\\theta}_2)^2}{1 + (x - \\hat{\\theta}_1)^2} = \\frac{1}{r} \\quad \\Rightarrow$$\n",
    "\n",
    "$$ r(1 + (x - \\hat{\\theta}_2)^2) = 1 + (x - \\hat{\\theta}_1)^2 \\quad \\Rightarrow \\quad (r-1)x^2 - 2(r\\hat{\\theta}_2 - \\hat{\\theta}_1)x + (r\\hat{\\theta}_2^2 + r - \\hat{\\theta}_1^2 - 1) = 0 $$\n",
    "\n",
    "The solutions to this quadratic equation define the decision boundary points. These points separate regions $\\mathbb{R}_1$ and $\\mathbb{R}_2$ in the feature space. If the equation above has two real solutions, $x_a$ and $x_b$, then one of the regions, $\\mathbb{R}_j$, will be an interval spanning $(-\\infty, x_a) \\cup (x_b, +\\infty)$, while the other will correspond to the interval $[x_a, x_b]$. We will specify these intervals after estimating the values of $\\hat{\\theta}_j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation\n",
    "\n",
    "Having outlined the theoretical approach to solving this problem, we now proceed with its implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_1$ and $D_2$ are the two example datasets provided. $D_1$ contains the values of the index $x$ for class $\\omega_1$, while $D_2$ contains the values for class $\\omega_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = np.array([2.8, -0.4, -0.8, 2.3, -0.3, 3.6, 4.1])  # No stress data (class omega_1)\n",
    "D2 = np.array([-4.5, -3.4, -3.1, -3.0, -2.3])  # Intense stress data (class omage_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement our classifier. The methods that are implemented are the following:\n",
    "\n",
    "1. `compute_pdf`: Computes the probability density function evaluation for given $\\theta$ and $x$ values\n",
    "2. `loglkhood`: Calculates the log-likelihood of a dataset $D$ given parameter $\\theta$\n",
    "3. `fit`: Finds the optimal $\\hat{\\theta}$ parameter that maximizes the log-likelihood for the given dataset\n",
    "4. `predict`: Predicts the class $\\omega$ by evaluating the discriminant function $g(x)$ using the fitted parameters and prior probabilities\n",
    "\n",
    "More detailed information about these methods can be found in the functions definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierA:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_pdf(theta, x) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the probability density function (PDF) evaluation for given theta and x values.\n",
    "        \n",
    "        For theta (M elements) and x (N elements), returns an M x N matrix where element (i,j)\n",
    "        represents the PDF evaluation at x[j] for theta[i]. \n",
    "        \n",
    "        Uses the distribution formula: p(x|θ) = 1/(π(1 + (x-θ)²))\n",
    "        \n",
    "        Args:\n",
    "            theta: Location parameter(s) of the distribution. Can be scalar or array.\n",
    "            x: Data point(s) to evaluate. Can be scalar or array.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Matrix of PDF evaluations with shape (M, N) where M is the number\n",
    "                        of theta values and N is the number of x values.\n",
    "        \"\"\"\n",
    "        # Compute differences using broadcasting\n",
    "        x, theta = map(np.atleast_1d, (x, theta))\n",
    "        diff = x[None, :] - theta[:, None]\n",
    "\n",
    "        # By simply modifying the returned PDF here, the same classifier\n",
    "        # can be utilized without requiring any additional changes (for the same requirements).\n",
    "        return 1.0 / (np.pi * (1.0 + diff * diff))\n",
    "\n",
    "    @staticmethod\n",
    "    def loglkhood(theta, D) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood of dataset D given parameter theta: l(θ).\n",
    "        \n",
    "        For M different theta values, returns an array of M log-likelihood values.\n",
    "        \n",
    "        Args:\n",
    "            theta: Location parameter(s) of the distribution. Can be scalar or array.\n",
    "            D: Dataset for which the log-likelihood is computed. Can be scalar or array.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Array of log-likelihood values for each theta. \n",
    "                        Shape: (M, 1) where M is the number of theta values.\n",
    "        \"\"\"\n",
    "        D, theta = map(np.atleast_1d, (D, theta))\n",
    "        \n",
    "        # Row-wise sum of the log of probabilities computed for each data point in D\n",
    "        # each row corresponds to a different theta value\n",
    "        return np.sum(np.log(ClassifierA.compute_pdf(theta, D)), axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(D, theta_min, theta_max, npoints=10000, plot=False, labels=0) -> float:\n",
    "        \"\"\"\n",
    "        Find the optimal theta parameter that maximizes the log-likelihood for the given dataset.\n",
    "\n",
    "        This function evaluates the log-likelihood for a range of theta values between\n",
    "        theta_min and theta_max, using npoints for precision. The theta value corresponding to\n",
    "        the maximum log-likelihood is selected as the optimal theta.\n",
    "\n",
    "        Optionally, a plot of the log-likelihood curve can be generated. The plot includes a marker\n",
    "        for the optimal theta value, with customized labels for different datasets (if specified).\n",
    "\n",
    "        Args:\n",
    "            D: Dataset for fitting the model. Can be scalar or array-like.\n",
    "            theta_min (float): Minimum value of theta for the search range.\n",
    "            theta_max (float): Maximum value of theta for the search range.\n",
    "            npoints (int, optional): Number of points to sample between theta_min and theta_max. \n",
    "                                    Default is 10000.\n",
    "            plot (bool, optional): If True, generates a plot of the log-likelihood curve. Default is False.\n",
    "            labels (int, optional): Dataset label for customizing plot annotations (1, 2, or others if needed). \n",
    "                                    Default is 0.\n",
    "\n",
    "        Returns:\n",
    "            float: The optimal theta value that maximizes the log-likelihood for the dataset.\n",
    "        \"\"\"\n",
    "        theta_candidates  = np.linspace(theta_min, theta_max, npoints)\n",
    "        lkhood_values     = ClassifierA.loglkhood(theta_candidates, D)\n",
    "        opt_theta         = theta_candidates[np.argmax(lkhood_values)]\n",
    "        \n",
    "        if plot:\n",
    "            # Determine appropriate label\n",
    "            labels = labels if labels else \"\"\n",
    "            \n",
    "            # Plot the log-likelihood curve:\n",
    "            plt.plot(theta_candidates, lkhood_values, label=rf'$\\log P(D{labels}|\\theta)$', \n",
    "                     color = 'blue' if labels == 1 else 'green')\n",
    "            # Mark the optimal theta value on the plot:\n",
    "            plt.scatter(opt_theta, ClassifierA.loglkhood(opt_theta, D), color='red', marker='x')\n",
    "            \n",
    "            plt.xlabel(rf'$\\theta{labels}$')\n",
    "            plt.ylabel('Log-likelihood')\n",
    "            plt.title(f'Log-likelihood plot for Part A, $\\omega{labels}$')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        return opt_theta\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(D, p1, p2, theta1, theta2) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class membership using the log-ratio discriminant function. \n",
    "        If theta1 is 1D array of M elements, theta2 is 1D array of K elements and \n",
    "        D is 1D array of N elements it returns a 2D array of max(M, K) x N elements \n",
    "        where the element at row i and column j is the prediction of feature D[j] corresponding \n",
    "        to theta1[i] and theta2[i] parameters.\n",
    "        \n",
    "        Args:\n",
    "            D: Dataset points for prediction. Can be scalar or array.\n",
    "            p1: Prior probability of class 1 (no stress). Must be in range (0, 1).\n",
    "            p2: Prior probability of class 2 (intense stress). Must be in range (0, 1).\n",
    "            theta1: Location parameter for class 1 distribution.\n",
    "                Can be scalar or array of M elements.\n",
    "            theta2: Location parameter for class 2 distribution.\n",
    "                Can be scalar or array of K elements.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Discriminant function values. Shape is (max(M,K), N) where:\n",
    "                        - M is the number of theta1 values\n",
    "                        - K is the number of theta2 values\n",
    "                        - N is the number of points in D\n",
    "                        Positive values indicate class 1, negative values indicate class 2.\n",
    "        \"\"\"\n",
    "        # Input validation check:\n",
    "        if not 0 < p1 < 1 or not 0 < p2 < 1:\n",
    "            raise ValueError(\"Prior probabilities must be between 0 and 1\")\n",
    "        if abs(p1 + p2 - 1) > 1e-5:\n",
    "            raise ValueError(\"Prior probabilities must sum to 1\")\n",
    "            \n",
    "        # g(x) = log(p(x|θ₁)) - log(p(x|θ₂)) + log(p₁) - log(p₂)\n",
    "        return (np.log(ClassifierA.compute_pdf(theta1, D)) - \n",
    "                np.log(ClassifierA.compute_pdf(theta2, D)) + \n",
    "                np.log(p1) - np.log(p2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Maximum Likelihood Estimation (Results)\n",
    "\n",
    "We execute the `fit` method for each dataset, $D_j$, to determine the optimal Maximum Likelihood estimations, $\\hat{\\theta}_j$. Additionally, we plot the log-likelihood function:\n",
    "\n",
    "$$ l(\\theta) = \\log P(D_j | \\theta), \\quad \\text{for} \\; j = 1, 2, $$\n",
    "\n",
    "and highlight the point where the likelihood reaches its maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ClassifierA()  # Create an instance of the Classifier\n",
    "theta1 = clf.fit(D1, -10, +10, npoints=10000, plot=True, labels=1)\n",
    "theta2 = clf.fit(D2, -10, +10, npoints=10000, plot=True, labels=2)\n",
    "print(f'theta1 ML estimation (no stress):      {theta1}')\n",
    "print(f'theta2 ML estimation (intense stress): {theta2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Bayes Decision Rule (Results)\n",
    "\n",
    "Based on the previously evaluated $\\hat{\\theta}_j$ results, we can now, before running the code, solve the equation $g(x) = 0$ and determine the two regions, $\\mathbb{R}_1$ and $\\mathbb{R}_2$, in the feature space. Specifically, we have found that $\\hat{\\theta}_1 \\simeq 2.6$ and $\\hat{\\theta}_2 \\simeq -3.16$.\n",
    "\n",
    "To solve the quadratic equation:\n",
    "\n",
    "$$ (r-1)x^2 - 2(r\\hat{\\theta}_2 - \\hat{\\theta}_1)x + (r\\hat{\\theta}_2^2 + r - \\hat{\\theta}_1^2 - 1) = 0, $$\n",
    "\n",
    "where $r = \\frac{7}{5}$, we can apply the quadratic formula and get the roots:\n",
    "\n",
    "$$ x_a \\approx −34.57, \\quad x_b \\approx −0.55.$$\n",
    "\n",
    "These values define the boundaries of the regions $\\mathbb{R}_1$ and $\\mathbb{R}_2$ in the feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate apriori probabilities for each class:\n",
    "N1 = len(D1)\n",
    "N2 = len(D2)\n",
    "p1 = N1 / (N1 + N2)\n",
    "p2 = N2 / (N1 + N2)\n",
    "\n",
    "# Get the discriminant values for the two classes:\n",
    "predictions1 = clf.predict(D1, p1, p2, theta1, theta2)\n",
    "predictions2 = clf.predict(D2, p1, p2, theta1, theta2)\n",
    "\n",
    "# Scatter plot of the data points and the discriminant function values:\n",
    "plt.scatter(D1, predictions1, label='no stress', color='blue', marker='o')\n",
    "plt.scatter(D2, predictions2, label='intense stress', color='green', marker='x')\n",
    "# Add a horizontal dashed line (threshold for classification):\n",
    "plt.axhline(y=0.0, color='red', linestyle='--', label=\"threshold\")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('g(x)')\n",
    "plt.title('Discriminant function values for D1, D2 datasets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we observe only one misclassification, where a sample that should have been classified as \"no stress\" was incorrectly predicted as \"intense stress.\" Additionally, it is evident that some \"no stress\" values are near the threshold.  \n",
    "\n",
    "It is important to note that, due to the very limited number of training samples, splitting $D_j$ into training and validation sets will not yield reliable results.  \n",
    "\n",
    "Now, we proceed to validate the intervals $\\mathbb{R}_1$ and $\\mathbb{R}_2$ by evaluating the predictor with specific values and checking the sign of the outcomes. While we lack a method to confirm whether these results are classified correctly, we will ensure that the intervals match those obtained theoretically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xValues = np.linspace(-50, 10, 100000)\n",
    "predictions = clf.predict(xValues, p1, p2, theta1, theta2)\n",
    "\n",
    "if len(predictions.shape) > 1:\n",
    "    predictions = predictions[0]  # Take first row if 2D array\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "\n",
    "# Plot points with positive predictions in red:\n",
    "plt.plot(xValues[predictions > 0], np.zeros_like(xValues[predictions > 0]), 'ro', \n",
    "         label='Class ω₁', markersize=1)\n",
    "\n",
    "# Plot points with negative predictions in blue:\n",
    "plt.plot(xValues[predictions <= 0], np.zeros_like(xValues[predictions <= 0]), 'bo', \n",
    "         label='Class ω₂', markersize=1)\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.grid(True, axis='x')\n",
    "plt.title('Decision Regions on the Real Line')\n",
    "plt.xlabel('x')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x_a = xValues[predictions <= 0][0]\n",
    "x_b = xValues[predictions <= 0][len(xValues[predictions <= 0]) - 1]\n",
    "\n",
    "print(f\"R1 interval: (-inf, {x_a}) and ({x_b}, +inf) --> class ω₁\")\n",
    "print(f\"R2 interval: [{x_a}, {x_b}]                  --> class ω₂\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "#### 1.\n",
    "\n",
    "The apriori PDF of $\\theta$ is given by:\n",
    "\n",
    "$$ p(\\theta) = \\frac{1}{10\\pi} \\frac{1}{1 + (\\theta / 10)^2} $$\n",
    "\n",
    "The likelihood $ p(D|\\theta) $ is computed by:\n",
    "\n",
    "$$ p(D_j|\\theta) = \\prod_{n=1}^{N_j} p(x_n|\\theta), \\quad j = 1, 2 \\quad. $$\n",
    "\n",
    "The a posteriori PDF will be:\n",
    "\n",
    "$$ p(\\theta|D_j) = \\frac{p(D_j|\\theta) p(\\theta)}{\\int p(D_j|\\theta) p(\\theta) \\, d\\theta}, \\quad j = 1, 2. $$\n",
    "\n",
    "#### 2.\n",
    "\n",
    "Consider Bayesian Estimation Rule:\n",
    "\n",
    "$$ p(\\omega_1 | x, D_1) > p(\\omega_2 | x, D_2) $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \n",
    "\\frac{p(x | D_1) P(\\omega_1)}{p(x | D_1) P(\\omega_1) + p(x | D_2) P(\\omega_2)} > \n",
    "\\frac{p(x | D_2) P(\\omega_2)}{p(x | D_1) P(\\omega_1) + p(x | D_2) P(\\omega_2)}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\log p(x | D_1) + \\log P(\\omega_1) > \\log p(x | D_2) + \\log P(\\omega_2) \\quad. $$\n",
    "\n",
    "By selecting as discriminant function the:\n",
    "\n",
    "$$ h(x) = \\log p(x | D_1) - \\log p(x | D_2) + \\log P(\\omega_1) - \\log P(\\omega_2) \\quad, $$\n",
    "\n",
    "we classify the element with feature $x$ to class $\\omega_1$ if $h(x) > 0$ and to $\\omega_2$ otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierB:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_pdf(theta, x):\n",
    "        \"\"\"\n",
    "        Compute the probability of x given a specific theta. If theta is a 1D array\n",
    "        of M elements and x is 1D array of N elements it returns a matrix M x N where\n",
    "        the element at row i and column j is the evaluation of the PDF at point x[j]\n",
    "        for theta[i].\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, location parameter of the distribution\n",
    "            x: Scalar or array, the data points to evaluate\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The probability for each x given theta\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)  # Ensure x is an array\n",
    "        theta = np.asarray(theta)  # Ensure theta is an array\n",
    "        # Convert theta to 1D array if it is a scalar\n",
    "        if theta.ndim == 0:\n",
    "            theta = np.array([theta])\n",
    "        return 1 / (1 + (x - theta[:, np.newaxis]) ** 2) / np.pi\n",
    "\n",
    "    @staticmethod\n",
    "    def p_theta(theta):\n",
    "        \"\"\"\n",
    "        Compute the apriori probability density function of theta.\n",
    "        If theta is a 1D array of M elements it returns a 1D array with\n",
    "        M elements with the evaluations of the function for each theta value.\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, the parameter values to evaluate.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The prior probability for each theta.\n",
    "        \"\"\"\n",
    "        theta = np.asarray(theta)  # Ensure theta is an array\n",
    "        return 1 / (1 + (theta / 10) ** 2) / (10 * np.pi)\n",
    "\n",
    "    @staticmethod\n",
    "    def p_D_theta(theta, D):\n",
    "        \"\"\"\n",
    "        Compute the likelihood of the dataset D given a parameter theta.\n",
    "        This is the product of individual probabilities p(x | theta) for all x in D.\n",
    "        If theta is a 1D array of M elements it returns a 1D array of M elements with\n",
    "        the likelihood values of the dataset D for each theta parameter.\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, the parameter values to evaluate.\n",
    "            D: Scalar of array, the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The likelihood values for each theta.\n",
    "        \"\"\"\n",
    "        # Computes the product of all elements in each row i.e. the likelihood of each theta given dataset D\n",
    "        return np.prod(ClassifierB.compute_pdf(theta, D), axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def p_theta_D(theta, D):\n",
    "        \"\"\"\n",
    "        Compute the posterior probability density function using Bayes' theorem.\n",
    "        If theta is a 1D array of M elements it returns a 1D array of M elements with the \n",
    "        evaluation of the posterior PDF at each theta.\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, the parameter values to evaluate.\n",
    "            D: Scalar or array, the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The posterior probability values for each theta.\n",
    "        \"\"\"\n",
    "        theta_max = 1000  # Range limit for theta\n",
    "        npoints = 5000  # Number of points for numerical integration\n",
    "        x = np.linspace(-theta_max, theta_max, npoints)  # Theta range for integration\n",
    "        y = ClassifierB.p_D_theta(x, D) * ClassifierB.p_theta(x)  # Unnormalized posterior\n",
    "        return ClassifierB.p_D_theta(theta, D) * ClassifierB.p_theta(theta) / np.trapz(y, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def p_x_D(x, D):\n",
    "        theta_max = 1000  # Range limit for theta\n",
    "        npoints = 5000  # Number of points for numerical integration\n",
    "        theta_values = np.linspace(-theta_max, theta_max, npoints)  # Theta range for integration\n",
    "\n",
    "        # Compute posterior pdf values accross the theta range and convert it to a column vector\n",
    "        posterior = ClassifierB.p_theta_D(theta_values, D)[:, np.newaxis]\n",
    "\n",
    "        # Compute the values of p(x | theta) * p(theta | D) accross the range of theta for each point x\n",
    "        # Each row corresponds to the same theta and each column to the same x\n",
    "        p_values = posterior * ClassifierB.compute_pdf(theta_values, x)\n",
    "        \n",
    "        # Integrate along the rows for each column to get the evaluation of p(x | D) at each point x\n",
    "        return np.trapz(p_values, theta_values, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(D, D1, D2, p1, p2):\n",
    "        \"\"\"\n",
    "        Predict the class of each data point by evaluating a discriminant function.\n",
    "        \n",
    "        Args:\n",
    "            D: Scalar or array, the dataset for which predictions are to be made\n",
    "            D1: Training dataset for class 1 (no stress)\n",
    "            D2: Training dataset for class 2 (intense stress)\n",
    "            p1: float, apriori probability of class 1 (no stress)\n",
    "            p2: float, apriori probability of class 2 (intense stress)\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The predicted discriminant function values for each data point\n",
    "        \"\"\"\n",
    "        return np.log(ClassifierB.p_x_D(D, D1)) - np.log(ClassifierB.p_x_D(D, D2)) + np.log(p1) - np.log(p2)\n",
    "\n",
    "\n",
    "clf = ClassifierB()  # Create an instance of the Classifier\n",
    "theta_max = 40  # Range limit for theta\n",
    "npoints = 1000  # Number of points to plot\n",
    "x = np.linspace(-theta_max, theta_max, npoints)  # Theta range for plotting\n",
    "y = clf.p_theta(x)  # Apriori pdf values for theta\n",
    "y1 = clf.p_theta_D(x, D1)  # A posteriori pdf values for theta given dataset D1\n",
    "y2 = clf.p_theta_D(x, D2)  # A posteriori pdf values for theta given dataset D2\n",
    "\n",
    "idx1 = np.argmax(y1)  # Find the index of the maximum value of the posterior pdf given D1\n",
    "idx2 = np.argmax(y2)  # Find the index of the maximum value of the posterior pdf given D2\n",
    "print(f'theta1 (no stress): {x[idx1]}')  # Print theta value that gives the maximum a posteriori pdf value given D1\n",
    "print(f'theta2 (intense stress): {x[idx2]}')  # Print theta value that gives the maximum a posteriori pdf value given D2\n",
    "\n",
    "# Plot prior and posterior pdf of theta\n",
    "plt.plot(x, y, label=r'$p(\\theta)$', color='red')\n",
    "plt.plot(x, y1, label=r'$p(\\theta|D1)$', color='blue')\n",
    "plt.plot(x, y2, label=r'$p(\\theta|D2)$', color='green')\n",
    "\n",
    "# Mark the peak points of the posterior pdfs\n",
    "plt.scatter(x[idx1], y1[idx1], color='blue', marker='x')\n",
    "plt.scatter(x[idx2], y2[idx2], color='green', marker='x')\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title(r'Apriori & a posteriori density functions of $\\theta$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Get the discriminant values for the two classes\n",
    "predictions1 = clf.predict(D1, D1, D2, p1, p2)\n",
    "predictions2 = clf.predict(D2, D1, D2, p1, p2)\n",
    "\n",
    "# Scatter plot of the data points and the discriminant function values\n",
    "plt.scatter(D1, predictions1, label='no stress', color='blue', marker='o')\n",
    "plt.scatter(D2, predictions2, label='intense stress', color='green', marker='x')\n",
    "\n",
    "# Add a horizontal dashed line (threshold for classification)\n",
    "plt.axhline(y=0.0, color='red', linestyle='--', label=\"threshold\")\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('h(x)')\n",
    "plt.title('Discriminant function values for D1, D2 datasets')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the a posteriori density functions $p(\\theta | D_j)$ are very sharp at $\\hat{\\theta}_j$, where\n",
    "$\\hat{\\theta}_j$ is very close to the ML estimation. Thus, the influence of the prior information on the \n",
    "uncertainty of the value of $\\theta$ can be ignored. \n",
    "\n",
    "It is evident that Bayesian Estimation gives us better results than ML estimation. This is beacause BE takes \n",
    "into account the prior distribution of the $\\theta$ parameter, leading to better solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris = load_iris()  # Load the Iris dataset\n",
    "rnd_seed = 42  # Random seed for reproducibility\n",
    "\n",
    "X = iris.data[:, :2]  # Extract the first two features of the dataset\n",
    "y = iris.target  # Get the target values of the dataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=rnd_seed)\n",
    "max_depth = 100  # Maximum depth of the tree\n",
    "depths = np.arange(1, max_depth + 1)  # Range of depths\n",
    "\n",
    "# ================================ Section 1 ================================\n",
    "\n",
    "accuracies_DT = np.zeros(max_depth)  # accuracy achieved for each depth of the decision tree\n",
    "\n",
    "# Test the accuracy of the DT for different tree depths\n",
    "for depth in depths:\n",
    "    # Create an instance of the classifier\n",
    "    clf = DecisionTreeClassifier(max_depth=depth, random_state=rnd_seed)\n",
    "    clf.fit(X_train, y_train)  # Train the classifier with the training data\n",
    "    y_pred = clf.predict(X_test)  # Find predictions of the model for the test set\n",
    "    accuracies_DT[depth - 1] = accuracy_score(y_test, y_pred)  # Calculate accuracy of the model\n",
    "\n",
    "best_depth_DT = np.argmax(accuracies_DT) + 1  # Find the depth of the tree that gives the best accuracy\n",
    "best_accuracy_DT = accuracies_DT[best_depth_DT - 1]  # Find the best accuracy\n",
    "print(f'Decision Tree: Best depth={best_depth_DT}, Accuracy={best_accuracy_DT}')\n",
    "\n",
    "# Create a meshgrid to plot decision boundaries\n",
    "npoints = 1000\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()  # Define x-axis range\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()  # Define y-axis range\n",
    "x_margin = 0.1 * (x_max - x_min)  # Define x-axis margin\n",
    "y_margin = 0.1 * (y_max - y_min)  # Define y-axis margin\n",
    "xx, yy = np.meshgrid(np.linspace(x_min - x_margin, x_max + x_margin, npoints), \n",
    "                     np.linspace(y_min - y_margin, y_max + y_margin, npoints))\n",
    "\n",
    "# Create an instance of the classifier with the optimal tree depth\n",
    "clf = DecisionTreeClassifier(max_depth=best_depth_DT, random_state=rnd_seed)\n",
    "clf.fit(X_train, y_train)  # Train the classifier with the training data\n",
    "\n",
    "# Predict the class for each point in the grid\n",
    "predictions = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # Use grid points as input\n",
    "predictions = predictions.reshape(xx.shape)  # Reshape predictions to match the grid's shape\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, predictions, alpha=0.8, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the training and testing points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='s', edgecolor='k', label='Train')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='o', edgecolor='k', label='Test')\n",
    "\n",
    "plt.title(f'Decision Boundaries for DT (depth: {best_depth_DT})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ================================ Section 2 ================================\n",
    "\n",
    "\n",
    "n_trees = 100  # Number of trees\n",
    "gamma = 0.5  # Fraction of the original training data to use for bootstrap sampling\n",
    "accuracies_RF = np.zeros(max_depth)  # Accuracy achieved by the Random Forest Classifier for each tree depth\n",
    "\n",
    "# Test the accuracy of the RF for different tree depths\n",
    "for depth in depths:\n",
    "    # Create an instance of the RF classifier\n",
    "    clf = RandomForestClassifier(n_estimators=n_trees, max_depth=depth, random_state=rnd_seed, bootstrap=True, max_samples=gamma, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)  # Train the classifier\n",
    "    y_pred = clf.predict(X_test)  # Make predictions\n",
    "    accuracies_RF[depth - 1] = accuracy_score(y_test, y_pred)  # Compute the accuracy\n",
    "\n",
    "best_depth_RF = np.argmax(accuracies_RF) + 1  # Find the depth of the tree that gives the best accuracy for RF\n",
    "best_accuracy_RF = accuracies_RF[best_depth_RF - 1]  # Find the best accuracy for RF\n",
    "print(f'Random Forest: Best depth={best_depth_RF}, Accuracy={best_accuracy_RF}')\n",
    "\n",
    "# Create an instance of the random forest classifier with the optimal depth\n",
    "clf = RandomForestClassifier(n_estimators=n_trees, max_depth=best_depth_RF, random_state=rnd_seed, bootstrap=True, max_samples=gamma, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)  # Train the classifier\n",
    "predictions = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # Make predictions, use as input the grid points\n",
    "predictions = predictions.reshape(xx.shape)  # Reshape predictions to match the grid's shape\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, predictions, alpha=0.8, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the training and testing points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='s', edgecolor='k', label='Train')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='o', edgecolor='k', label='Test')\n",
    "\n",
    "plt.title(f'Decision Boundaries for RF (depth: {best_depth_RF})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy versus the depth of the tree for DT and RF\n",
    "plt.plot(depths, accuracies_DT, label='DT', color='red')\n",
    "plt.plot(depths, accuracies_RF, label='RF', color='blue')\n",
    "plt.title('Accuracy vs Depth for DT & RF')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "gammas = np.linspace(0.1, 1, 20)  # gamma parameter range\n",
    "accuracies_RF = np.zeros(gammas.size)\n",
    "\n",
    "# Test the accuracy of the RF for different values of the gamma parameter\n",
    "# for fixed tree depth equal to the optimal\n",
    "for i in range(0, gammas.size):\n",
    "    # Create an instance of the RF classifier\n",
    "    clf = RandomForestClassifier(n_estimators=n_trees, max_depth=best_depth_RF, random_state=rnd_seed, bootstrap=True, max_samples=gammas[i], n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)  # Train the classifier\n",
    "    y_pred = clf.predict(X_test)  # Make predictions\n",
    "    accuracies_RF[i] = accuracy_score(y_test, y_pred)  # Compute the accuracy\n",
    "\n",
    "# Plot accuracy versus gamma parameter of the RF for fixed tree depth\n",
    "plt.plot(gammas, accuracies_RF)\n",
    "plt.title(f'Accuracy vs gamma for RF (depth: {best_depth_RF})')\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
