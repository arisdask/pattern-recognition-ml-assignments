{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "#### 1.\n",
    "\n",
    "The PDF of stress factor x is given by:\n",
    "\n",
    "$$ p(x|\\theta) = \\frac{1}{\\pi}\\frac{1}{1 + (x - \\theta)^2} \\quad, $$\n",
    "\n",
    "where theta is an unknown constant. We assume that the samples $ D_j , j = 1, 2 $ are Independent Identically Distributed\n",
    "(i.i.d.) i.e.  they have been drawn independently from the same distribution $ p(x|\\theta_j, \\omega_j) $. Thus, the \n",
    "common PDF from the samples is:\n",
    "\n",
    "$$ p(D_j | \\theta_j) = \\prod_{i=1}^{N_j} p(x_i | \\theta_j), \\quad j = 1, 2 \\quad. $$\n",
    "\n",
    "We will use the Maximum Likelihood (ML) Method to estimate the parameters $\\hat{\\theta}_1, \\hat{\\theta}_2$. Therefore, \n",
    "we want to maximize the log-likelihood function:\n",
    "\n",
    "$$ L(D_j | \\theta_j) = \\sum_{i=1}^{N_j} \\log p(x_i | \\theta_j), \\quad j = 1, 2 \\quad, $$\n",
    "\n",
    "with respect to $ \\theta_j $.\n",
    "\n",
    "#### 2.\n",
    "\n",
    "Consider the Bayes Decision Rule:\n",
    "\n",
    "$$ P(\\omega_1 | x) > P(\\omega_2 | x) $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\frac{p(x|\\omega_1) P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) P(\\omega_2)}{p(x)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\log p(x|\\omega_1) + \\log P(\\omega_1) > \\log p(x|\\omega_2) + \\log P(\\omega_2), $$\n",
    "\n",
    "where the class conditional densities $p(x|\\omega_1), p(x|\\omega_2)$ have already been computed using ML estimation. \n",
    "By selecting as discriminant function the:\n",
    "\n",
    "$$ g(x) = \\log p(x|\\omega_1) - \\log p(x|\\omega_2) + \\log P(\\omega_1) - \\log P(\\omega_2) \\quad, $$\n",
    "\n",
    "we classify the element with feature $x$ to class $\\omega_1$ if $g(x) > 0$ and to $\\omega_2$ otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "D1 = np.array([2.8, -0.4, -0.8, 2.3, -0.3, 3.6, 4.1])  # No stress data\n",
    "D2 = np.array([-4.5, -3.4, -3.1, -3.0, -2.3])  # Intense stress data\n",
    "\n",
    "class ClassifierA:\n",
    "\n",
    "    @staticmethod\n",
    "    def p_x_theta(theta, x):\n",
    "        \"\"\"\n",
    "        Compute the probability of x given a specific theta. If theta is a 1D array\n",
    "        of M elements and x is 1D array of N elements it returns a matrix M x N where\n",
    "        the element at row i and column j is the evaluation of the PDF at point x[j]\n",
    "        for theta[i].\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, location parameter of the distribution\n",
    "            x: Scalar or array, the data points to evaluate\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The probability for each x given theta\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)  # Ensure x is an array\n",
    "        theta = np.asarray(theta)  # Ensure theta is an array\n",
    "        # Convert theta to 1D array if it is a scalar\n",
    "        if theta.ndim == 0:\n",
    "            theta = np.array([theta])\n",
    "        return 1 / (1 + (x - theta[:, np.newaxis]) ** 2) / np.pi\n",
    "\n",
    "    @staticmethod\n",
    "    def loglkhood(theta, D):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the data D given the parameter theta.\n",
    "        If theta is a 1D array of M elements it returns a 1D array of M elements,\n",
    "        that is the log-likelihood of the data D for each theta value.\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, the parameter for the distribution\n",
    "            D: Scalar or array, the dataset for which the log-likelihood is computed\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The log-likelihood of the data given theta\n",
    "        \"\"\"\n",
    "        # Row-wise sum of the log of probabilities computed for each data point in D\n",
    "        # each row corresponds to a different theta value\n",
    "        return np.sum(np.log(ClassifierA.p_x_theta(theta, D)), axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(D, theta_canditates):\n",
    "        \"\"\"\n",
    "        Fit the model by finding the theta that maximizes the log-likelihood function \n",
    "        (minimizing the negative log-likelihood).\n",
    "        \n",
    "        Args:\n",
    "            D: Scalar or array, the dataset\n",
    "            theta_candidates: Array of candidate thetas to search for the optimal theta\n",
    "        \n",
    "        Returns:\n",
    "            float: The optimal theta value that maximizes the log-likelihood for the dataset\n",
    "        \"\"\"\n",
    "        fun_values = np.zeros(len(theta_canditates))  # Array to store function values\n",
    "        theta_values = np.zeros(len(theta_canditates))  # Array to store corresponding theta values\n",
    "\n",
    "        # Loop over candidate thetas, and minimize the negative log-likelihood\n",
    "        for i, x0 in enumerate(theta_canditates):\n",
    "            # Maximize log-likelihood by minimizing the negation of it using a lambda function\n",
    "            result = minimize(lambda theta: -ClassifierA.loglkhood(theta, D), x0, method='BFGS')\n",
    "            theta_values[i] = result.x[0]  # Best theta value for the specific starting point\n",
    "            fun_values[i] = result.fun  # Negative log-likelihood corresponding to the best theta\n",
    "\n",
    "        idx = np.argmin(fun_values)  # Find the index with the minimum function value\n",
    "        return theta_values[idx]  # Return the optimal theta\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(D, p1, p2, theta1, theta2):\n",
    "        \"\"\"\n",
    "        Predict the class of each data point by evaluating a discriminant function.\n",
    "        If theta1 is 1D array of M elements, theta2 is 1D array of K elements and \n",
    "        D is 1D array of N elements it returns a 2D array of max(M, K) x N elements \n",
    "        where the element at row i and column j is the prediction of D[j] corresponding \n",
    "        to theta1[i] and theta2[i] parameters.\n",
    "        \n",
    "        Args:\n",
    "            D: Scalar or array, the dataset for which predictions are to be made\n",
    "            p1: float, apriori probability of class 1 (no stress)\n",
    "            p2: float, apriori probability of class 2 (intense stress)\n",
    "            theta1: Scalar or array, optimal theta for class 1 (no stress)\n",
    "            theta2: Scalar or array, optimal theta for class 2 (intense stress)\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The predicted discriminant function values for each data point\n",
    "        \"\"\"\n",
    "        return np.log(ClassifierA.p_x_theta(theta1, D)) - np.log(ClassifierA.p_x_theta(theta2, D)) + np.log(p1) - np.log(p2)\n",
    "\n",
    "\n",
    "clf = ClassifierA()  # Create an instance of the Classifier\n",
    "# Candidate thetas for D1, on the interval where the optimal theta lies, as observed from the plot\n",
    "theta_candidates = np.linspace(-5., 5., 10)\n",
    "theta1 = clf.fit(D1, theta_candidates)\n",
    "# Candidate thetas for D2, on the interval where the optimal theta lies, as observed from the plot\n",
    "theta_candidates = np.linspace(-5., 0., 10)\n",
    "theta2 = clf.fit(D2, theta_candidates)\n",
    "print(f'theta1 (no stress): {theta1}')\n",
    "print(f'theta2 (intense stress): {theta2}')\n",
    "\n",
    "# Compute log-likelihood values for both classes\n",
    "theta_max = 20\n",
    "npoints = 1000\n",
    "x = np.linspace(-theta_max, theta_max, npoints)  # Range for theta values\n",
    "y1 = clf.loglkhood(x, D1)  # Log-likelihood for D1\n",
    "y2 = clf.loglkhood(x, D2)  # Log-likelihood for D2\n",
    "\n",
    "# Plot the log-likelihood curves for both datasets\n",
    "plt.plot(x, y1, label=r'$logP(D1|\\theta)$ (no stress)', color='blue')\n",
    "plt.plot(x, y2, label=r'$logP(D2|\\theta)$ (intense stress)', color='green')\n",
    "\n",
    "# Mark the optimal theta values on the plot\n",
    "plt.scatter(theta1, clf.loglkhood(theta1, D1), color='blue', marker='x')\n",
    "plt.scatter(theta2, clf.loglkhood(theta2, D2), color='green', marker='x')\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.title('Log-likelihood plot for Part A')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate apriori probabilities for each class\n",
    "N1 = len(D1)\n",
    "N2 = len(D2)\n",
    "p1 = N1 / (N1 + N2)\n",
    "p2 = N2 / (N1 + N2)\n",
    "\n",
    "# Get the discriminant values for the two classes\n",
    "predictions1 = clf.predict(D1, p1, p2, theta1, theta2)\n",
    "predictions2 = clf.predict(D2, p1, p2, theta1, theta2)\n",
    "\n",
    "# Scatter plot of the data points and the discriminant function values\n",
    "plt.scatter(D1, predictions1, label='no stress', color='blue', marker='o')\n",
    "plt.scatter(D2, predictions2, label='intense stress', color='green', marker='x')\n",
    "\n",
    "# Add a horizontal dashed line (threshold for classification)\n",
    "plt.axhline(y=0.0, color='red', linestyle='--', label=\"threshold\")\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('g(x)')\n",
    "plt.title('Discriminant function values for D1, D2 datasets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "#### 1.\n",
    "\n",
    "The apriori PDF of $\\theta$ is given by:\n",
    "\n",
    "$$ p(\\theta) = \\frac{1}{10\\pi} \\frac{1}{1 + (\\theta / 10)^2} $$\n",
    "\n",
    "The likelihood $ p(D|\\theta) $ is computed by:\n",
    "\n",
    "$$ p(D_j|\\theta) = \\prod_{n=1}^{N_j} p(x_n|\\theta), \\quad j = 1, 2 \\quad. $$\n",
    "\n",
    "The a posteriori PDF will be:\n",
    "\n",
    "$$ p(\\theta|D_j) = \\frac{p(D_j|\\theta) p(\\theta)}{\\int p(D_j|\\theta) p(\\theta) \\, d\\theta}, \\quad j = 1, 2. $$\n",
    "\n",
    "#### 2.\n",
    "\n",
    "Consider Bayesian Estimation Rule:\n",
    "\n",
    "$$ p(\\omega_1 | x, D_1) > p(\\omega_2 | x, D_2) $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \n",
    "\\frac{p(x | D_1) P(\\omega_1)}{p(x | D_1) P(\\omega_1) + p(x | D_2) P(\\omega_2)} > \n",
    "\\frac{p(x | D_2) P(\\omega_2)}{p(x | D_1) P(\\omega_1) + p(x | D_2) P(\\omega_2)}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\log p(x | D_1) + \\log P(\\omega_1) > \\log p(x | D_2) + \\log P(\\omega_2) \\quad. $$\n",
    "\n",
    "By selecting as discriminant function the:\n",
    "\n",
    "$$ h(x) = \\log p(x | D_1) - \\log p(x | D_2) + \\log P(\\omega_1) - \\log P(\\omega_2) \\quad, $$\n",
    "\n",
    "we classify the element with feature $x$ to class $\\omega_1$ if $h(x) > 0$ and to $\\omega_2$ otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierB:\n",
    "\n",
    "    @staticmethod\n",
    "    def p_x_theta(theta, x):\n",
    "        \"\"\"\n",
    "        Compute the probability of x given a specific theta. If theta is a 1D array\n",
    "        of M elements and x is 1D array of N elements it returns a matrix M x N where\n",
    "        the element at row i and column j is the evaluation of the PDF at point x[j]\n",
    "        for theta[i].\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, location parameter of the distribution\n",
    "            x: Scalar or array, the data points to evaluate\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The probability for each x given theta\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)  # Ensure x is an array\n",
    "        theta = np.asarray(theta)  # Ensure theta is an array\n",
    "        # Convert theta to 1D array if it is a scalar\n",
    "        if theta.ndim == 0:\n",
    "            theta = np.array([theta])\n",
    "        return 1 / (1 + (x - theta[:, np.newaxis]) ** 2) / np.pi\n",
    "\n",
    "    @staticmethod\n",
    "    def p_theta(theta):\n",
    "        \"\"\"\n",
    "        Compute the apriori probability density function of theta.\n",
    "        If theta is a 1D array of M elements it returns a 1D array with\n",
    "        M elements with the evaluations of the function for each theta value.\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, the parameter values to evaluate.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The prior probability for each theta.\n",
    "        \"\"\"\n",
    "        theta = np.asarray(theta)  # Ensure theta is an array\n",
    "        return 1 / (1 + (theta / 10) ** 2) / (10 * np.pi)\n",
    "\n",
    "    @staticmethod\n",
    "    def p_D_theta(theta, D):\n",
    "        \"\"\"\n",
    "        Compute the likelihood of the dataset D given a parameter theta.\n",
    "        This is the product of individual probabilities p(x | theta) for all x in D.\n",
    "        If theta is a 1D array of M elements it returns a 1D array of M elements with\n",
    "        the likelihood values of the dataset D for each theta parameter.\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, the parameter values to evaluate.\n",
    "            D: Scalar of array, the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The likelihood values for each theta.\n",
    "        \"\"\"\n",
    "        # Computes the product of all elements in each row i.e. the likelihood of each theta given dataset D\n",
    "        return np.prod(ClassifierB.p_x_theta(theta, D), axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def p_theta_D(theta, D):\n",
    "        \"\"\"\n",
    "        Compute the posterior probability density function using Bayes' theorem.\n",
    "        If theta is a 1D array of M elements it returns a 1D array of M elements with the \n",
    "        evaluation of the posterior PDF at each theta.\n",
    "        \n",
    "        Args:\n",
    "            theta: Scalar or array, the parameter values to evaluate.\n",
    "            D: Scalar or array, the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The posterior probability values for each theta.\n",
    "        \"\"\"\n",
    "        theta_max = 1000  # Range limit for theta\n",
    "        npoints = 5000  # Number of points for numerical integration\n",
    "        x = np.linspace(-theta_max, theta_max, npoints)  # Theta range for integration\n",
    "        y = ClassifierB.p_D_theta(x, D) * ClassifierB.p_theta(x)  # Unnormalized posterior\n",
    "        return ClassifierB.p_D_theta(theta, D) * ClassifierB.p_theta(theta) / np.trapz(y, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def p_x_D(x, D):\n",
    "        theta_max = 1000  # Range limit for theta\n",
    "        npoints = 5000  # Number of points for numerical integration\n",
    "        theta_values = np.linspace(-theta_max, theta_max, npoints)  # Theta range for integration\n",
    "\n",
    "        # Compute posterior pdf values accross the theta range and convert it to a column vector\n",
    "        posterior = ClassifierB.p_theta_D(theta_values, D)[:, np.newaxis]\n",
    "\n",
    "        # Compute the values of p(x | theta) * p(theta | D) accross the range of theta for each point x\n",
    "        # Each row corresponds to the same theta and each column to the same x\n",
    "        p_values = posterior * ClassifierB.p_x_theta(theta_values, x)\n",
    "        \n",
    "        # Integrate along the rows for each column to get the evaluation of p(x | D) at each point x\n",
    "        return np.trapz(p_values, theta_values, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(D, D1, D2, p1, p2):\n",
    "        \"\"\"\n",
    "        Predict the class of each data point by evaluating a discriminant function.\n",
    "        \n",
    "        Args:\n",
    "            D: Scalar or array, the dataset for which predictions are to be made\n",
    "            D1: Training dataset for class 1 (no stress)\n",
    "            D2: Training dataset for class 2 (intense stress)\n",
    "            p1: float, apriori probability of class 1 (no stress)\n",
    "            p2: float, apriori probability of class 2 (intense stress)\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: The predicted discriminant function values for each data point\n",
    "        \"\"\"\n",
    "        return np.log(ClassifierB.p_x_D(D, D1)) - np.log(ClassifierB.p_x_D(D, D2)) + np.log(p1) - np.log(p2)\n",
    "\n",
    "\n",
    "clf = ClassifierB()  # Create an instance of the Classifier\n",
    "theta_max = 40  # Range limit for theta\n",
    "npoints = 1000  # Number of points to plot\n",
    "x = np.linspace(-theta_max, theta_max, npoints)  # Theta range for plotting\n",
    "y = clf.p_theta(x)  # Apriori pdf values for theta\n",
    "y1 = clf.p_theta_D(x, D1)  # A posteriori pdf values for theta given dataset D1\n",
    "y2 = clf.p_theta_D(x, D2)  # A posteriori pdf values for theta given dataset D2\n",
    "\n",
    "idx1 = np.argmax(y1)  # Find the index of the maximum value of the posterior pdf given D1\n",
    "idx2 = np.argmax(y2)  # Find the index of the maximum value of the posterior pdf given D2\n",
    "print(f'theta1 (no stress): {x[idx1]}')  # Print theta value that gives the maximum a posteriori pdf value given D1\n",
    "print(f'theta2 (intense stress): {x[idx2]}')  # Print theta value that gives the maximum a posteriori pdf value given D2\n",
    "\n",
    "# Plot prior and posterior pdf of theta\n",
    "plt.plot(x, y, label=r'$p(\\theta)$', color='red')\n",
    "plt.plot(x, y1, label=r'$p(\\theta|D1)$', color='blue')\n",
    "plt.plot(x, y2, label=r'$p(\\theta|D2)$', color='green')\n",
    "\n",
    "# Mark the peak points of the posterior pdfs\n",
    "plt.scatter(x[idx1], y1[idx1], color='blue', marker='x')\n",
    "plt.scatter(x[idx2], y2[idx2], color='green', marker='x')\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title(r'Apriori & a posteriori density functions of $\\theta$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Get the discriminant values for the two classes\n",
    "predictions1 = clf.predict(D1, D1, D2, p1, p2)\n",
    "predictions2 = clf.predict(D2, D1, D2, p1, p2)\n",
    "\n",
    "# Scatter plot of the data points and the discriminant function values\n",
    "plt.scatter(D1, predictions1, label='no stress', color='blue', marker='o')\n",
    "plt.scatter(D2, predictions2, label='intense stress', color='green', marker='x')\n",
    "\n",
    "# Add a horizontal dashed line (threshold for classification)\n",
    "plt.axhline(y=0.0, color='red', linestyle='--', label=\"threshold\")\n",
    "\n",
    "# Labeling the plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('h(x)')\n",
    "plt.title('Discriminant function values for D1, D2 datasets')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the a posteriori density functions $p(\\theta | D_j)$ are very sharp at $\\hat{\\theta}_j$, where\n",
    "$\\hat{\\theta}_j$ is very close to the ML estimation. Thus, the influence of the prior information on the \n",
    "uncertainty of the value of $\\theta$ can be ignored. \n",
    "\n",
    "It is evident that Bayesian Estimation gives us better results than ML estimation. This is beacause BE takes \n",
    "into account the prior distribution of the $\\theta$ parameter, leading to better solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris = load_iris()  # Load the Iris dataset\n",
    "rnd_seed = 42  # Random seed for reproducibility\n",
    "\n",
    "X = iris.data[:, :2]  # Extract the first two features of the dataset\n",
    "y = iris.target  # Get the target values of the dataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=rnd_seed)\n",
    "max_depth = 100  # Maximum depth of the tree\n",
    "depths = np.arange(1, max_depth + 1)  # Range of depths\n",
    "\n",
    "# ================================ Section 1 ================================\n",
    "\n",
    "accuracies_DT = np.zeros(max_depth)  # accuracy achieved for each depth of the decision tree\n",
    "\n",
    "# Test the accuracy of the DT for different tree depths\n",
    "for depth in depths:\n",
    "    # Create an instance of the classifier\n",
    "    clf = DecisionTreeClassifier(max_depth=depth, random_state=rnd_seed)\n",
    "    clf.fit(X_train, y_train)  # Train the classifier with the training data\n",
    "    y_pred = clf.predict(X_test)  # Find predictions of the model for the test set\n",
    "    accuracies_DT[depth - 1] = accuracy_score(y_test, y_pred)  # Calculate accuracy of the model\n",
    "\n",
    "best_depth_DT = np.argmax(accuracies_DT) + 1  # Find the depth of the tree that gives the best accuracy\n",
    "best_accuracy_DT = accuracies_DT[best_depth_DT - 1]  # Find the best accuracy\n",
    "print(f'Decision Tree: Best depth={best_depth_DT}, Accuracy={best_accuracy_DT}')\n",
    "\n",
    "# Create a meshgrid to plot decision boundaries\n",
    "npoints = 1000\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()  # Define x-axis range\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()  # Define y-axis range\n",
    "x_margin = 0.1 * (x_max - x_min)  # Define x-axis margin\n",
    "y_margin = 0.1 * (y_max - y_min)  # Define y-axis margin\n",
    "xx, yy = np.meshgrid(np.linspace(x_min - x_margin, x_max + x_margin, npoints), \n",
    "                     np.linspace(y_min - y_margin, y_max + y_margin, npoints))\n",
    "\n",
    "# Create an instance of the classifier with the optimal tree depth\n",
    "clf = DecisionTreeClassifier(max_depth=best_depth_DT, random_state=rnd_seed)\n",
    "clf.fit(X_train, y_train)  # Train the classifier with the training data\n",
    "\n",
    "# Predict the class for each point in the grid\n",
    "predictions = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # Use grid points as input\n",
    "predictions = predictions.reshape(xx.shape)  # Reshape predictions to match the grid's shape\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, predictions, alpha=0.8, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the training and testing points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='s', edgecolor='k', label='Train')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='o', edgecolor='k', label='Test')\n",
    "\n",
    "plt.title(f'Decision Boundaries for DT (depth: {best_depth_DT})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ================================ Section 2 ================================\n",
    "\n",
    "\n",
    "n_trees = 100  # Number of trees\n",
    "gamma = 0.5  # Fraction of the original training data to use for bootstrap sampling\n",
    "accuracies_RF = np.zeros(max_depth)  # Accuracy achieved by the Random Forest Classifier for each tree depth\n",
    "\n",
    "# Test the accuracy of the RF for different tree depths\n",
    "for depth in depths:\n",
    "    # Create an instance of the RF classifier\n",
    "    clf = RandomForestClassifier(n_estimators=n_trees, max_depth=depth, random_state=rnd_seed, bootstrap=True, max_samples=gamma, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)  # Train the classifier\n",
    "    y_pred = clf.predict(X_test)  # Make predictions\n",
    "    accuracies_RF[depth - 1] = accuracy_score(y_test, y_pred)  # Compute the accuracy\n",
    "\n",
    "best_depth_RF = np.argmax(accuracies_RF) + 1  # Find the depth of the tree that gives the best accuracy for RF\n",
    "best_accuracy_RF = accuracies_RF[best_depth_RF - 1]  # Find the best accuracy for RF\n",
    "print(f'Random Forest: Best depth={best_depth_RF}, Accuracy={best_accuracy_RF}')\n",
    "\n",
    "# Create an instance of the random forest classifier with the optimal depth\n",
    "clf = RandomForestClassifier(n_estimators=n_trees, max_depth=best_depth_RF, random_state=rnd_seed, bootstrap=True, max_samples=gamma, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)  # Train the classifier\n",
    "predictions = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # Make predictions, use as input the grid points\n",
    "predictions = predictions.reshape(xx.shape)  # Reshape predictions to match the grid's shape\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, predictions, alpha=0.8, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the training and testing points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='s', edgecolor='k', label='Train')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='o', edgecolor='k', label='Test')\n",
    "\n",
    "plt.title(f'Decision Boundaries for RF (depth: {best_depth_RF})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy versus the depth of the tree for DT and RF\n",
    "plt.plot(depths, accuracies_DT, label='DT', color='red')\n",
    "plt.plot(depths, accuracies_RF, label='RF', color='blue')\n",
    "plt.title('Accuracy vs Depth for DT & RF')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "gammas = np.linspace(0.1, 1, 20)  # gamma parameter range\n",
    "accuracies_RF = np.zeros(gammas.size)\n",
    "\n",
    "# Test the accuracy of the RF for different values of the gamma parameter\n",
    "# for fixed tree depth equal to the optimal\n",
    "for i in range(0, gammas.size):\n",
    "    # Create an instance of the RF classifier\n",
    "    clf = RandomForestClassifier(n_estimators=n_trees, max_depth=best_depth_RF, random_state=rnd_seed, bootstrap=True, max_samples=gammas[i], n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)  # Train the classifier\n",
    "    y_pred = clf.predict(X_test)  # Make predictions\n",
    "    accuracies_RF[i] = accuracy_score(y_test, y_pred)  # Compute the accuracy\n",
    "\n",
    "# Plot accuracy versus gamma parameter of the RF for fixed tree depth\n",
    "plt.plot(gammas, accuracies_RF)\n",
    "plt.title(f'Accuracy vs gamma for RF (depth: {best_depth_RF})')\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
